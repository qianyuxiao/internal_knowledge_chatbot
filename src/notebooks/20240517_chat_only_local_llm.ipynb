{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a522b955-ee68-4522-8177-15035d59b09d",
   "metadata": {},
   "source": [
    "# Get Retriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cad56f00-b908-4dbb-ac5a-518d8ef36984",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Add the directory to sys.path\n",
    "sys.path.append('/home/qianyucazelles/internal_knowledge_chatbot/src/utils')\n",
    "from doc_to_vertex_search import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a80b01fa-b7ff-42d9-83c9-aa2e4a765c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = get_me_parameters('../vector_store_me_parameters/ecg_all_me.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b1e33ee-622e-4a76-bb44-bcf0ff1da961",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = get_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e211639c-e954-41f2-80c8-1194bee57552",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = get_vector_store(parameters, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98f7e621-29b1-4691-96fb-4eaec134e168",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud.aiplatform.matching_engine.matching_engine_index_endpoint import (\n",
    "    Namespace,\n",
    "    NumericNamespace,\n",
    ")\n",
    "filters = [Namespace(name=\"department\", allow_tokens=[\"dpo\"]),\n",
    "           Namespace(name=\"loader_method\", allow_tokens=[\"unstructured\"])\n",
    "          ]\n",
    "\n",
    "\n",
    "numeric_filters = [NumericNamespace(name=\"chunk_size\", value_float=2000.0, op=\"EQUAL\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e741dbdf-7f80-4f2d-996b-aa51bf9ddf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b528dbf8-c72a-4c89-83e6-bf8dc9d6faba",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.search_kwargs = {\"filter\": filters, \"numeric_filter\": numeric_filters}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c9b19fd0-31eb-42c6-a2f5-10c02111416b",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"Je n'aurai pas le temps de traiter la demande de droit à l'oubli d'un utilisateur dans le mois règlementaire imposé par la CNIL. Ai-je un recours ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6bd9bffd-71b0-45c2-8a3d-0a21479216eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "q2 = \"J'ai des difficultés à comprendre les différentes obligations que j'ai vis-à-vis des sous-traitants qui travaillent pour moi. Pourrais-tu me faire un résumé ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2730bea8-2b5d-476b-bb4f-01fe622b944a",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_documents = retriever.invoke(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "78e9b375-e1f4-402c-8abd-9e406e426681",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_documents = retriever.invoke(q2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5cb06a",
   "metadata": {},
   "source": [
    "## local llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "535fbba8-5189-4dc5-9a4e-23ae13193664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/blog/not-lain/rag-chatbot-using-llama3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a37d0de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c7e53b9-6cc2-4414-9972-2dc6f6e77fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qianyucazelles/internal_knowledge_chatbot/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████| 4/4 [00:06<00:00,  1.55s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "# use quantization to lower GPU usage\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8edd48f-f05f-4a37-8591-8012aaf3810c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6e16ff79",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYS_PROMPT = \"\"\"You are an assistant for answering questions.\n",
    "You are given the extracted parts of several documents rank by relevance with document name and a question. Please pick the most relevant document related to the question then \n",
    "provide a conversational answer in the same language as the question. Always cite the referred document in your response.\n",
    "If you don't know the answer, just say \"I do not know.\" Don't make up an answer.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "356db936-5000-4b06-aff1-982cf739cddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(prompt,retrieved_documents):\n",
    "  \"\"\"using the retrieved documents we will prompt the model to generate our responses\"\"\"\n",
    "  formatted_prompt = f\"Question:{prompt}\\nContext: \\n\"\n",
    "  separated_line = \"-\"*50+\"\\n\"\n",
    "  for idx,doc in enumerate(retrieved_documents) :\n",
    "    formatted_prompt+= f\"{separated_line} Ranked Document: {idx+1} \\nName: {doc.metadata['document_name']}\\nContent: {doc.page_content} \\n\"\n",
    "  return formatted_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "91e11636-9a23-4a60-b3f3-b678a0aef406",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_prompt = format_prompt(q2,retrieved_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "247e5e05-b7db-4156-ab56-2c28f78ea599",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{\"role\":\"system\",\"content\":SYS_PROMPT},{\"role\":\"user\",\"content\":formatted_prompt}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d5d6eba8-2323-4a7a-b2f5-9922b7431f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextIteratorStreamer\n",
    "\n",
    "from threading import Thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "cb9e3e84-fd3e-452a-9adc-18122ddb2f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set streamer\n",
    "streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# apply chat template\n",
    "messages_tmpl = tokenizer.apply_chat_template(messages,\n",
    "                       tokenize=False,\n",
    "                       add_generation_prompt=True\n",
    "                      )\n",
    "\n",
    "# tokenize msgs\n",
    "inputs = tokenizer([messages_tmpl], return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "# Run the generation in a separate thread, so that we can fetch the generated text in a non-blocking way.\n",
    "generation_kwargs = dict(inputs, \n",
    "                         streamer=streamer, \n",
    "                         max_new_tokens=1500,\n",
    "                         eos_token_id=terminators)\n",
    "# chat\n",
    "thread = Thread(target=model.generate,\n",
    "                kwargs=generation_kwargs)\n",
    "thread.start() \n",
    "\n",
    "generated_text = ''\n",
    "for _, new_text in enumerate(streamer):\n",
    "    generated_text += new_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "cdb74f34-c070-4252-bbcc-aff05dd6a049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"J'ai des difficultés à comprendre les différentes obligations que j'ai vis-à-vis des sous-traitants qui travaillent pour moi. Pourrais-tu me faire un résumé ?\""
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "60302e28-efd5-4fed-8fbb-613f955730f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Bonjour! Je comprends que vous avez des difficultés à comprendre les différentes obligations que vous avez vis-à-vis des sous-traitants qui travaillent pour vous.\\n\\nSelon le chapitre IV du Règlement européen sur la protection des données, un sous-traitant doit informer immédiatement le responsable du traitement si, selon lui, une instruction constitue une violation du présent règlement ou d'autres dispositions du droit de l'Union ou du droit des États membres relatives à la protection des données (Article 28, h).\\n\\nEn outre, le sous-traitant doit prendre des garanties suffisantes pour la mise en œuvre de mesures techniques et organisationnelles appropriées pour que le traitement réponde aux exigences du présent règlement. Si l'autre sous-traitant ne remplit pas ses obligations en matière de protection des données, le sous-traitant initial demeure pleinement responsable devant le responsable du traitement de l'exécution par l'autre sous-traitant de ses obligations (Article 28, 4).\\n\\nEn résumé, vous devez vous assurer que les sous-traitants qui travaillent pour vous prennent des garanties suffisantes pour la protection des données et que vous êtes informé de tout changement prévu concernant l'ajout ou le remplacement d'autres sous-traitants.\\n\\nCitation : Article 28 du Règlement européen sur la protection des données, chapitre IV - Responsable du traitement et sous-traitant, CNIL.\""
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_text"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m119",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m119"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
