{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78cff29f-1fe2-4ce3-8306-b1a97903b7dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dec805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set credentials\n",
    "# gcloud auth application-default login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e551f47-f620-4b69-bbf4-c24a6e2c9e76",
   "metadata": {},
   "source": [
    "# Vertex parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "025d33b8-0d34-457b-97c2-06faaff41733",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.doc_to_vertex_search import get_me_parameters\n",
    "file_path = \"me_parameters.json\"\n",
    "parameters =  get_me_parameters(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "019fbf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = parameters['PROJECT_ID']\n",
    "LOCATION = parameters['LOCATION']\n",
    "CHATBOT_NAME = parameters['CHATBOT_NAME']\n",
    "ME_INDEX_ID = parameters['ME_INDEX_ID']\n",
    "ME_INDEX_ENDPOINT_ID = parameters['ME_INDEX_ENDPOINT_ID']\n",
    "ME_INDEX_NAME = parameters['ME_INDEX_NAME']\n",
    "ME_EMBEDDING_DIR = parameters['ME_EMBEDDING_DIR']\n",
    "ME_DIMENSIONS = parameters['ME_DIMENSIONS']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f439b6-0930-4f0f-9e27-00808b29d52f",
   "metadata": {},
   "source": [
    "# Initiate Embeddings, vector store and retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7185e0b5-9391-4821-bf88-c2a21e13bdd5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model_name will become a required arg for VertexAIEmbeddings starting from Feb-01-2024. Currently the default is set to textembedding-gecko@001\n"
     ]
    }
   ],
   "source": [
    "# initiate embeddings\n",
    "from utils.custom_vertexai_embeddings import CustomVertexAIEmbeddings\n",
    "# Embeddings API integrated with langChain\n",
    "EMBEDDING_QPM = 100\n",
    "EMBEDDING_NUM_BATCH = 5\n",
    "embeddings = CustomVertexAIEmbeddings(\n",
    "    requests_per_minute=EMBEDDING_QPM,\n",
    "    num_instances_per_batch=EMBEDDING_NUM_BATCH,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5fddb982-8d17-4ab9-81d9-77ef2a7ae7af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initiate vector store\n",
    "from utils.matching_engine import MatchingEngine\n",
    "me = MatchingEngine.from_components(\n",
    "    project_id=PROJECT_ID,\n",
    "    region=LOCATION,\n",
    "    gcs_bucket_name=f\"gs://{ME_EMBEDDING_DIR}\".split(\"/\")[2],\n",
    "    embedding=embeddings,\n",
    "    index_id=ME_INDEX_ID,\n",
    "    endpoint_id=ME_INDEX_ENDPOINT_ID,\n",
    "    # credentials_path=credentials_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b506a427-5829-432b-92b2-7a51ddd2b79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test whether search from vector store is working\n",
    "# me.similarity_search(\"moyens de réserver un séjour\", k=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23cad3f4-0020-4686-a257-0515baf3dd0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Expose index to the retriever\n",
    "NUMBER_OF_RESULTS = 2\n",
    "SEARCH_DISTANCE_THRESHOLD = 0.6\n",
    "\n",
    "retriever = me.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\n",
    "        \"k\": NUMBER_OF_RESULTS,\n",
    "        \"search_distance\": SEARCH_DISTANCE_THRESHOLD,\n",
    "    },\n",
    "    filters=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdcb5c0-8262-4ade-9679-177762930818",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# qn = \"What're the methods to reserve a stay?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ddfc1c-5483-4091-b808-057d42e07f78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# retriever.get_relevant_documents(qn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523e85ff-8606-4508-b8ce-7cb27dbeab1c",
   "metadata": {},
   "source": [
    "# Initiate llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5cb06a",
   "metadata": {},
   "source": [
    "## local llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a37d0de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e16ff79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qianyucazelles/chatbot_rag/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-04-02 15:46:41.727197: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Loading checkpoint shards: 100%|██████████| 8/8 [01:12<00:00,  9.00s/it]\n"
     ]
    }
   ],
   "source": [
    "#ref: https://medium.com/aimonks/exploring-offline-rag-with-langchain-zephyr-7b-beta-and-decilm-7b-c0626e09ee1f\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM,BitsAndBytesConfig, BitsAndBytesConfig\n",
    "from transformers import TextStreamer\n",
    "\n",
    "torch_dtype = torch.float16\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.set_device(device)\n",
    "\n",
    "model_id = \"HuggingFaceH4/zephyr-7b-beta\" #\"google/gemma-2b-it\"\n",
    "\n",
    "#download model\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "#                                                 torch_dtype=torch_dtype, \n",
    "#                                                 low_cpu_mem_usage=True,\n",
    "#                                                 #quantization_config= quantization_config\n",
    "#                                                 )\n",
    "# model.save_pretrained('zephyr-7b-beta-model', max_shard_size=\"1000MB\")\n",
    "# tokenizer.save_pretrained('zephyr-7b-beta-tokenizer')\n",
    "# del model\n",
    "# del tokenizer\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdbff877",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 15/15 [01:01<00:00,  4.12s/it]\n"
     ]
    }
   ],
   "source": [
    "# load it from the local folder:\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"zephyr-7b-beta-tokenizer\",torch_dtype=torch_dtype)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"zephyr-7b-beta-model\", low_cpu_mem_usage=True, torch_dtype=torch_dtype,quantization_config= quantization_config)\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "929d8498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref: https://github.com/langchain-ai/langchain/issues/2918\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=1026,\n",
    "    do_sample= True,\n",
    "    temperature=0.1,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.2,\n",
    "    streamer=streamer\n",
    ")\n",
    "\n",
    "gpu_llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b48241",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(gpu_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa81a257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Apr  2 15:55:22 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            On   | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   77C    P0    35W /  70W |   9220MiB / 15360MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla T4            On   | 00000000:00:05.0 Off |                    0 |\n",
      "| N/A   76C    P0    33W /  70W |   8554MiB / 15360MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   1679127      C   /opt/conda/bin/python3.10        9216MiB |\n",
      "|    1   N/A  N/A     86425      C   ...atbot_rag/venv/bin/python     8550MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15205df-f2d0-420b-a0d4-2733bf064452",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Initiate template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e5b9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gemme-2b-it template https://medium.com/@mohammed97ashraf/building-a-retrieval-augmented-generation-rag-model-with-gemma-and-langchain-a-step-by-step-f917fc6f753f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d4b66f3-482d-41d3-b8ea-7c63e6ccfc1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "template = \"\"\"Given the following conversation and a follow-up question, rephrase the follow-up question to be a standalone question, in its original English.\n",
    "                        Chat History:\n",
    "                        {chat_history}\n",
    "                        Follow-Up Input: {question}\n",
    "                        Standalone question:\"\"\"\n",
    "CUSTOM_QUESTION_PROMPT = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670061d7-9631-421e-91f1-1e45550a321c",
   "metadata": {},
   "source": [
    "# Initiate memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94cbe334-6ba9-4150-b604-00423cc3da39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set memory\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=f\"chat_history\",\n",
    "    return_messages=True,\n",
    "    output_key='answer'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eaa19a4-5485-4db0-b8ad-c597917dc73d",
   "metadata": {},
   "source": [
    "# Set chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b80f569-d774-488a-a948-d4e782850e57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8732862c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "template = \"\"\"SYSTEM: You are an intelligent assistant helping the users with their questions on papers related to laws conditions and always cite the article that you're referring to.\n",
    "Please alway reply at the same language that question uses.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Strictly Use ONLY the following pieces of context to answer the question at the end. Think step-by-step and then answer.\n",
    "\n",
    "Do not try to make up an answer:\n",
    " - If the answer to the question cannot be determined from the context alone, say \"I cannot determine the answer to that.\"\n",
    " - If the context is empty, just say \"I do not know the answer to that.\"\n",
    "\n",
    "=============\n",
    "{context}\n",
    "=============\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba3c6d0b-f08d-4399-879a-0600dc541f90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "        llm=gpu_llm, \n",
    "        retriever=retriever,\n",
    "        chain_type=\"stuff\",\n",
    "        memory=memory,\n",
    "        combine_docs_chain_kwargs={\"prompt\": QA_CHAIN_PROMPT},\n",
    "        return_source_documents=True,\n",
    "        return_generated_question=True #to solve error\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac2fc00-ece1-481b-9b78-33874e112e04",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb18066d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference to get helpful answer\n",
    "# https://huggingface.co/spaces/cvachet/pdf-chatbot/blob/main/app.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "608e961e-e816-423f-a215-b13da8719560",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "memory.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4974cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = \"What are the methods to reserve a stay?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bbd43e6c-3589-4c13-b4ac-ee708de4e238",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting\n",
      "There are four ways to reserve a stay: by phone through our reservation teams using the numbers provided below (local call charges), for Homair Vacances at 04.84.39.08.60 and for Tohapi at 04.48.20.20.20 or +33 4 48 20 20 20 when calling from abroad; through the company's websites; by email using the following addresses: for Homair Vacances at https://contact.homair.com/hc/fr/requests/new, for Tohapi at reservations@tohapi.fr and for Marvilla Parks at https://contact.marvilla -parks.com/hc/fr/requests/new; directly at the campsite reception desk for Homair Vacances campsites only. The reservation process follows these steps: first, the customer selects the desired campsite based on its description; second, the customer specifies the duration of the stay, the departure date, the number of occupants and the type of accommodation or pitch; third, the customer enters any benefits received, providing relevant codes where applicable (partner code or promotional code); fourth, the customer has the option to subscribe to cancellation insurance and reserve other services proposed by the company. When making a reservation by phone, the customer should provide all necessary details to the reservation agent. For online reservations, the customer completes the mandatory fields marked with an asterisk on the reservation form on the website. Finally, the customer books in the name and on behalf of all those who will benefit from the stay.</s>\n"
     ]
    }
   ],
   "source": [
    "response = qa.invoke({\"question\": message})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd72ae61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Apr  2 15:57:44 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            On   | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   68C    P0    32W /  70W |   9220MiB / 15360MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla T4            On   | 00000000:00:05.0 Off |                    0 |\n",
      "| N/A   77C    P0    34W /  70W |  13894MiB / 15360MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   1679127      C   /opt/conda/bin/python3.10        9216MiB |\n",
      "|    1   N/A  N/A     86425      C   ...atbot_rag/venv/bin/python    13890MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8271cf82-96d4-4f7c-822b-35cc0667f93f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = qa.invoke({\"question\": \"Can you give more details?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e181dafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e943b78-34d6-42ec-8baa-f39c405e9b8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# response = qa({\"question\": \"I mean details about how to reserve a stay?\"})"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m119",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m119"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
